# -*- coding: utf-8 -*-
"""ddpm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Cv39Ryg3Mvqs3oOxxbCmGl4NZZB6o-P
"""

import os
from google.colab import drive
drive.mount('/content/drive')
os.chdir('/content/drive/My Drive/MNIST/DDPM')

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

# Parameter values
num_timesteps = 1000
beta_start = 0.0001
beta_end = 0.02
betas = np.linspace(beta_start, beta_end, num_timesteps, dtype = np.float32)

# Compute alpha coefficients
alphas = 1.0 - betas
alpha_bar = np.cumprod(alphas)
alpha_bar_tf = tf.convert_to_tensor(alpha_bar, dtype = tf.float32)

def forward_diffusion_process(x0, t):
    batch_size = tf.shape(x0)[0]
    noise = tf.random.normal(tf.shape(x0))
    sqrt_alpha_bar_t = tf.gather(tf.sqrt(alpha_bar_tf), t)
    sqrt_one_minus_alpha_bar_t = tf.gather(tf.sqrt(1.0 - alpha_bar_tf), t)
    xt = sqrt_alpha_bar_t[:, None, None, None] * x0 + sqrt_one_minus_alpha_bar_t[:, None, None, None] * noise
    return xt, noise

def build_unet_with_timestep(input_shape):
    image_input = layers.Input(shape = input_shape)
    t_input = layers.Input(shape = (), dtype = tf.int32)

    # Embed the time step
    t_embedding = layers.Embedding(input_dim = num_timesteps, output_dim = 64)(t_input)
    t_embedding = layers.Dense(np.prod(input_shape))(t_embedding)
    t_embedding = layers.Reshape(input_shape)(t_embedding)

    # Combine the input image and time step embeddings
    x = layers.Concatenate()([image_input, t_embedding])

    # Encoder
    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = layers.MaxPooling2D((2, 2))(conv1)

    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = layers.MaxPooling2D((2, 2))(conv2)

    # Bottleneck
    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)
    # conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)         # Added later
    # conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)         # Added later

    # Decoder
    up1 = layers.UpSampling2D((2, 2))(conv3)
    concat1 = layers.Concatenate()([up1, conv2])
    conv4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(concat1)
    conv4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv4)

    up2 = layers.UpSampling2D((2, 2))(conv4)
    concat2 = layers.Concatenate()([up2, conv1])
    conv5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(concat2)
    conv5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv5)

    outputs = layers.Conv2D(1, (1, 1))(conv5)

    model = models.Model([image_input, t_input], outputs)
    return model

# a = build_unet_with_timestep((28, 28, 1))
# a.summary()

# Model and optimizer
input_shape = (28, 28, 1)  # MNIST images are grayscale
model = build_unet_with_timestep(input_shape)
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4)

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
# x_train = (x_train.astype('float32') - 127.5) / 127.5                # changed from /255
x_train = x_train.astype('float32')/255                # changed from /255

x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension
# x_test = (x_test.astype('float32') - 127.5) / 127.5                  # changed from /255
x_test = x_test.astype('float32')/255                  # changed from /255

x_test = np.expand_dims(x_test, axis=-1)  # Add channel dimension

# Create a data loader
train_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size = 32)     # Changed from batch_size = 32
val_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size = 32)

# Define callbacks
checkpoint_path = "/content/drive/My Drive/MNIST/DDPM/best_model.keras"
csv_logger_path = "/content/drive/My Drive/MNIST/DDPM/training.csv"

save_best_model = callbacks.ModelCheckpoint(checkpoint_path,
                                        monitor='val_loss',
                                        mode='min',
                                        save_best_only=True, save_weights_only = True,
                                        verbose=1)
save_best_model.model = model
#class SaveBestModel(callbacks.Callback):
#    def __init__(self, model_file):
#        super(SaveBestModel, self).__init__()
#        self.model_file = model_file
#        self.best_val_loss = np.Inf

#    def on_epoch_end(self, epoch, logs=None):
#        if logs is None:
#            logs = {}
#        val_loss = logs.get('val_loss', np.Inf)
#        if val_loss < self.best_val_loss:
#            self.best_val_loss = val_loss
#            self.model.save(self.model_file)
#            print(f"Saved model with val_loss: {val_loss}")

#save_best_model = SaveBestModel(checkpoint_path)
csv_logger = callbacks.CSVLogger(filename=csv_logger_path, append=True)
csv_file = open(csv_logger_path, 'a')
csv_logger.csv_file = csv_file

# define MSE loss function
def loss_fn(model, x0, t):
    xt, noise = forward_diffusion_process(x0, t)
    noise_pred = model([xt, t], training=True)
    return tf.reduce_mean(tf.keras.losses.MSE(noise, noise_pred))

@tf.function
def train_step(x0, t):
    with tf.GradientTape() as tape:
         loss = loss_fn(model, x0, t)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

# Training loop
num_epochs = 200              # changed from 100
for epoch in range(num_epochs):
    for batch in train_dataset:
        x0 = batch
        t = tf.random.uniform([tf.shape(x0)[0]], minval = 0, maxval = num_timesteps, dtype = tf.int32)
        train_loss = train_step(x0, t)

    # Evaluate on validation data
    val_losses = []
    for batch in val_dataset:
        x0 = batch
        t = tf.random.uniform([tf.shape(x0)[0]], minval=0, maxval=num_timesteps, dtype=tf.int32)
        val_loss = loss_fn(model, x0, t)
        val_losses.append(val_loss)
    val_loss = tf.reduce_mean(val_losses)

    print(f"Epoch {epoch + 1}, Train Loss: {train_loss.numpy()}, Val Loss: {val_loss.numpy()}")

    # Save model weights and log training progress
    save_best_model.on_epoch_end(epoch, {"val_loss": val_loss.numpy()})
    # model.save_weights(checkpoint_path)
    csv_logger.on_epoch_end(epoch, {"loss": train_loss.numpy(), "val_loss": val_loss.numpy()})

m_path = '/content/drive/My Drive/MNIST/DDPM/model.keras'
model.save(m_path)

import tensorflow as tf
# Save the trained model
model_path = "/content/drive/My Drive/MNIST/DDPM/model.keras"
weights_path = "/content/drive/My Drive/MNIST/DDPM/best_model.keras"
# os.makedirs(model_path, exist_ok=True)  # Create the directory if it doesn't exist
# file_path = os.path.join(model_path, 'model.keras')
loaded_model = tf.keras.models.load_model(model_path)
loaded_model.load_weights(weights_path)
# Save the model weights
# weights_path = "/content/drive/My Drive/MNIST/DDPM/weights.h5"
# model.save_weights(weights_path)

import numpy as np

num_timesteps = 1000
beta_start = 0.0001
beta_end = 0.02
betas = np.linspace(beta_start, beta_end, num_timesteps, dtype = np.float32)

# Compute alpha coefficients
alphas = 1.0 - betas
alpha_bar = np.cumprod(alphas)
alpha_bar_tf = tf.convert_to_tensor(alpha_bar, dtype = tf.float32)

# Sample Generation
def sample(model, num_samples, num_timesteps):
    x = tf.random.normal((num_samples, 28, 28, 1))
    y = tf.identity(x)
    for t in reversed(range(num_timesteps)):
        t_tensor = tf.convert_to_tensor([t] * num_samples, dtype=tf.int32)
        noise_pred = model([x, t_tensor])
        alpha_t = alphas[t]
        alpha_t_bar = tf.gather(alpha_bar_tf, t)
        alpha_t_minus_one_bar = tf.gather(alpha_bar_tf, t - 1)
        beta_t = betas[t]
        sigma_t = tf.sqrt(beta_t * (1.0 - alpha_t_minus_one_bar) / (1.0 - alpha_t_bar))
        if t > 0:
           noise = tf.random.normal(tf.shape(x))
        else:
           noise = tf.zeros_like(x)
        x = (x - beta_t/tf.sqrt(1.0 - alpha_t_bar) * noise_pred)/tf.sqrt(alpha_t) + noise * sigma_t
    return x, y

import matplotlib.pyplot as plt
# Generate samples
num_samples = 49  # Number of samples to generate
num_timesteps = 1000
generated_samples, noise_samples = sample(loaded_model, num_samples, num_timesteps)

# Reshape and scale the samples to [0, 1] range
generated_samples = (generated_samples + 1) / 2  # Scale from [-1, 1] to [0, 1]
generated_samples = generated_samples.numpy().reshape(num_samples, 28, 28)
noise_samples = (noise_samples + 1) / 2  # Scale from [-1, 1] to [0, 1]
noise_samples = noise_samples.numpy().reshape(num_samples, 28, 28)

# Plot the generated samples
fig, axes = plt.subplots(nrows=7, ncols=7)
for ax, img in zip(axes.flatten(), generated_samples):
    ax.imshow(img, cmap='gray')
    ax.axis('off')
plt.tight_layout()
fig.text(0.02, 0.5, 't = 1000', fontsize=16, va='center', rotation='vertical')  # Add the vertically tilted title
plt.savefig('/content/drive/My Drive/MNIST/DDPM/generated_image.png')
plt.show()

# Plot the generated samples
fig, axes = plt.subplots(nrows=7, ncols=7)
for ax, img in zip(axes.flatten(), noise_samples):
    ax.imshow(img, cmap='gray')
    ax.axis('off')
plt.tight_layout()
fig.text(0.02, 0.5, 't = 0', fontsize=16, va='center', rotation='vertical')  # Add the vertically tilted title
plt.savefig('/content/drive/My Drive/MNIST/DDPM/noise.png')
plt.show()